import csv
import time
import folium
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from geopy.geocoders import Nominatim

# å•Ÿå‹• Selenium ç€è¦½å™¨
driver = webdriver.Chrome()
base_url = "https://www.104.com.tw/jobs/search/?indcat=1001000000&isJobList=1&jobsource=joblist_search&page"
page_count = 1  # èµ·å§‹é æ•¸
tale_url= "&keyword=python&area=6001008000"
fina_url = f"{base_url}{page_count}{tale_url}"
driver.get(fina_url)

# è¨­å®šæœ€å¤šæŠ“å–çš„è·ç¼ºæ•¸é‡
max_jobs = 40
collected_jobs = 0
max_pages = 3 # é™åˆ¶ç¿»é æ¬¡æ•¸


wait = WebDriverWait(driver, 10)

# å­˜æ”¾è·ç¼ºè³‡è¨Šçš„åˆ—è¡¨
job_list = []

# å»ºç«‹åœ°åœ–
job_map = folium.Map(location=[23.6978, 120.9605], zoom_start=7)  # å°ç£ä¸­å¿ƒé»
geolocator = Nominatim(user_agent="job_locator")

# **æŠ“å–è³‡æ–™ä¸¦ç¿»é **  
while collected_jobs < max_jobs and page_count <= max_pages:
    time.sleep(3)  # ç­‰å¾…é é¢è¼‰å…¥
    
    # è§£æ HTML
    soup = BeautifulSoup(driver.page_source, "lxml")
    job_blocks = soup.select("div.info")  # æ‰¾åˆ°æ‰€æœ‰è·ç¼ºå€å¡Š

    for job in job_blocks[:20]:  # æ¯é æœ€å¤š 20 ç­†
        title_element = job.select_one("a[data-gtm-joblist='è·ç¼º-è·ç¼ºåç¨±']")
        title = title_element.get("title", "æœªæä¾›è·ç¼ºåç¨±").strip() if title_element else "æœªæä¾›è·ç¼ºåç¨±"
        link = title_element.get("href", "#") if title_element else "#"

        location_element = job.select_one("a[data-gtm-joblist^='è·ç¼º-åœ°å€-']")
        location = location_element.get_text(strip=True) if location_element else "æœªæä¾›åœ°å€"

        salary_element = job.select_one("a[data-gtm-joblist^='è·ç¼º-è–ªè³‡-']")
        salary = salary_element.get_text(strip=True) if salary_element else "æœªæä¾›è–ªè³‡"

        if "/job/" in link and title:  # ç¢ºä¿æœ‰æ¨™é¡Œå’Œé€£çµ
            print(f"è·ç¼ºåç¨±: {title}")
            print(f"è·ç¼ºç¶²å€: {link}")
            print(f"åœ°é»: {location}")
            print(f"è–ªè³‡: {salary}")
            print("-" * 50)

            job_list.append([title, link, location, salary])
            collected_jobs += 1
            print(f"ğŸ” å·²æ”¶é›† {collected_jobs} ç­† / ç›®æ¨™ {max_jobs} ç­†")

            # **è½‰æ›åœ°é»ç‚ºç¶“ç·¯åº¦**
            try:
                location_data = geolocator.geocode(location)
                if location_data:
                    lat, lon = location_data.latitude, location_data.longitude
                    folium.Marker(
                        [lat, lon],
                        popup=f"{title}<br>{location}<br>{salary}<br><a href='{link}' target='_blank'>è·ç¼ºé€£çµ</a>",
                        tooltip=title,
                        icon=folium.Icon(color="blue", icon="briefcase"),
                    ).add_to(job_map)
                else:
                    print(f"âš ï¸ æ‰¾ä¸åˆ°åœ°é»: {location}")
            except Exception as e:
                print(f"âš ï¸ åœ°ç†ç·¨ç¢¼éŒ¯èª¤: {e}")

        if collected_jobs >= max_jobs:
            break

    # åªåœ¨è³‡æ–™æŠ“å–å®Œç•¢å¾Œé€²è¡Œç¿»é 
    if collected_jobs < max_jobs:
        page_count += 1
        

# é—œé–‰ç€è¦½å™¨
driver.quit()

# **å„²å­˜ CSV æª”æ¡ˆ**
csv_filename = "job_list.csv"
df = pd.DataFrame(job_list, columns=["è·ç¼ºåç¨±", "è·ç¼ºç¶²å€", "åœ°é»", "è–ªè³‡"])
df.to_csv(csv_filename, index=False, encoding="utf-8-sig")
print(f"âœ… CSV æª”æ¡ˆå·²å„²å­˜ç‚º {csv_filename}")

# **å„²å­˜åœ°åœ–**
job_map.save("job_map.html")
print("âœ… åœ°åœ–å·²å„²å­˜ç‚º job_map.html")
